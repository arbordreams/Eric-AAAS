{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Medical TokAlign on RunPod (H100)\n",
        "End-to-end: setup -> data prep -> tokenizer adaptation -> evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, os\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "print('CUDA device:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU')\n",
        "os.environ.setdefault('CUDA_VISIBLE_DEVICES','0')\n",
        "os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF','expandable_segments:True')\n",
        "os.environ.setdefault('HF_HOME','/workspace/.cache/huggingface')\n",
        "# RunPod often sets HUGGINGFACE_TOKEN; alias to HF_TOKEN for libraries\n",
        "if 'HUGGINGFACE_TOKEN' in os.environ and 'HF_TOKEN' not in os.environ:\n",
        "    os.environ['HF_TOKEN'] = os.environ['HUGGINGFACE_TOKEN']\n",
        "    os.environ['HUGGINGFACE_HUB_TOKEN'] = os.environ['HUGGINGFACE_TOKEN']\n",
        "print('HF_HOME:', os.environ['HF_HOME'])\n",
        "print('HF_TOKEN set:', 'HF_TOKEN' in os.environ)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install core requirements\n",
        "!pip install -r medical_tokalign/requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare datasets (standard splits)\n",
        "!bash medical_tokalign/scripts/prepare_medical_data.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run tokenizer adaptation (TokAlign-style full adaptation)\n",
        "!bash medical_tokalign/scripts/run_vocab_adaptation.sh --model_id meta-llama/Meta-Llama-3.1-8B --top_k 8192\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate (vLLM backend by default)\n",
        "!bash medical_tokalign/scripts/eval_medical.sh --config medical_tokalign/configs/eval_medical.yaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display latest metrics\n",
        "import os, glob, json\n",
        "runs = sorted(glob.glob('medical_tokalign/runs/medical_eval/*'))\n",
        "print('Runs:', runs[-3:])\n",
        "if runs:\n",
        "    path = os.path.join(runs[-1], 'metrics.json')\n",
        "    print('Latest metrics:', path)\n",
        "    print(json.dumps(json.load(open(path)), indent=2))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
